{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential for Meta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let we train a model $f(\\cdot;w)$ with data $x$, a loss function $\\mathcal{L}$ and a learning rate $\\alpha$. Then we can derive following equations about the weight of model will be $w_T$, the weight after $T$-th iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_T = w_{T-1} - \\alpha \\nabla_{w_{T-1}} \\mathcal{L} (f(x;w_{T-1}))$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial}{\\partial x} \\mathcal{L} (f(x;w_{T}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a normal situation, we consider $w_T$ is a independent variable for $x$. PyTorch also adopts this framework through nn.Module or nn.Parameters. However, in a meta learning framwork, we want to know a gradient of gradient. That is, in the second eqaution, **you have to note that $w_T$ is also a function of $x$** if you do not detach them. In formula, $\\frac{\\partial}{\\partial x} \\mathcal{L} (f(x;w_{T}(x)))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider following situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x;w)=(wx)^2$, $\\mathcal{L}(f(x;w))=2-(wx)^2$, $\\alpha=0.1$, $w_0=1$, $x=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, $\\frac{\\partial \\mathcal{L}}{\\partial w} = -2wx^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we train this function with two iterations.\n",
    "\n",
    "(1) 1st iteration :\n",
    "\n",
    "$w_1 = w_0 + 0.1\\cdot 2 w_0 \\cdot x^2 = w_0 + 0.2w_0x^2 = 1.2$\n",
    "\n",
    "(2) 2nd iteration :\n",
    "\n",
    "$w_2 = w_1 + 0.1\\cdot 2 w_1 \\cdot x^2$\n",
    "\n",
    "$= (w_0 + 0.2w_0x^2) + 0.2(w_0 + 0.2w_0x^2)x^2$\n",
    "\n",
    "$= w_0 + 0.4w_0x^2 + 0.04w_0x^4 = 1.44$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the gradient of $x$ for each iteration would be as follows :\n",
    "\n",
    "(1) 1st iteration :\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} \\mathcal{L} (f(x;w_{0})) = \\frac{\\partial}{\\partial x} \\{ 2 - (w_0x)^2\\} = -2w_0^2x = -2$$\n",
    "\n",
    "(2) 2nd iteration :\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x} \\mathcal{L} (f(x;w_{1})) = \\frac{\\partial}{\\partial x} \\{ 2 - (w_1x)^2\\}$$\n",
    "$$= - \\frac{\\partial}{\\partial x} (\\{w_0 + 0.2w_0x^2\\}x)^2$$\n",
    "$$= - \\frac{\\partial}{\\partial x} (w_0x + 0.2w_0x^3)^2$$\n",
    "$$= - 2(w_0x + 0.2w_0x^3)(w_0 + 0.6w_0x^2)$$\n",
    "$$= -2\\cdot1.2\\cdot1.6 = -3.84$$\n",
    "\n",
    "Again, note that $\\frac{\\partial}{\\partial x} \\mathcal{L} (f(x;w_{1})) \\neq -2w_1^2x = -2.88$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will see the difference between two frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Normal Situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(1, 1, bias=False)\n",
    "        self.fc.weight.data.uniform_(1,1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.]])\n",
    "Y = torch.tensor([[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 th iteration]\n",
      "Grad: tensor([[-2.]])\n",
      "Weight: Parameter containing:\n",
      "tensor([[1.2000]], requires_grad=True)\n",
      "\n",
      "[ 1 th iteration]\n",
      "Grad: tensor([[-2.8800]])\n",
      "Weight: Parameter containing:\n",
      "tensor([[1.4400]], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(2) :\n",
    "    print(\"[\",j, \"th iteration]\")\n",
    "    \n",
    "    pre = model(x)\n",
    "    cost = nn.L1Loss()(pre, Y)\n",
    "    \n",
    "    grad = torch.autograd.grad(cost, x, retain_graph=True)[0]\n",
    "    print(\"Grad:\", grad)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "        \n",
    "    print(\"Weight:\",model.fc.weight)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that PyTorch considers $w_T$ as a independent variable for $x$. Then, how about we save the params in each iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module(model, names):\n",
    "    if len(names) == 0 :\n",
    "        return model\n",
    "    name = names[0]\n",
    "    del names[0]\n",
    "    return get_module(getattr(model, name), names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(1, 1, bias=False)\n",
    "        self.fc.weight.data.uniform_(1,1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.]])\n",
    "Y = torch.tensor([[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 th iteration]\n",
      "Grad: tensor([[-2.]])\n",
      "Weight: OrderedDict([('fc.weight', Parameter containing:\n",
      "tensor([[1.2000]], requires_grad=True))])\n",
      "\n",
      "[ 1 th iteration]\n",
      "Grad: tensor([[-2.8800]])\n",
      "Weight: OrderedDict([('fc.weight', Parameter containing:\n",
      "tensor([[1.4400]], requires_grad=True))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_param = collections.OrderedDict(model.named_parameters())\n",
    "params = []\n",
    "params.append(current_param.copy())\n",
    "\n",
    "for j in range(2) :\n",
    "    print(\"[\",j, \"th iteration]\")\n",
    "    \n",
    "    # RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n",
    "    # Load_state_dict will only change the \"value\" of params.\n",
    "    # model.load_state_dict(params[j])\n",
    "    \n",
    "    for key in params[j].keys() :\n",
    "        a = key.split(\".\")\n",
    "        setattr(get_module(model, a[:-1]), a[-1], params[j][key])\n",
    "    \n",
    "    pre = model(x)\n",
    "    cost = nn.L1Loss()(pre, Y)\n",
    "    \n",
    "    grad = torch.autograd.grad(cost, x, retain_graph=True)[0]\n",
    "    print(\"Grad:\", grad)\n",
    "    \n",
    "    # If you use below lines, it changes the original params\n",
    "    # so that the gradient calculation will be inaccurate.\n",
    "    \n",
    "    # optimizer = optim.SGD(params[j].values(), lr=0.1)\n",
    "    # optimizer.zero_grad()\n",
    "    # cost.backward(retain_graph=True)\n",
    "    # optimizer.step()\n",
    "\n",
    "    grads = torch.autograd.grad(cost, params[j].values(), \n",
    "                                retain_graph=True, create_graph=True)\n",
    "\n",
    "    params.append(params[j].copy())\n",
    "    \n",
    "    for i, key in enumerate(current_param.keys()):\n",
    "        params[j+1][key] = nn.Parameter(params[j+1][key] - 0.1*grads[i])\n",
    "\n",
    "    print(\"Weight:\",params[j+1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same results. The problem is \"nn.Parameter\". If you use nn.Module, which is a very obvious choice for normal cituation, the Module will assign a nn.Paramter for weights. One of **the characteristic of nn.Parameter is that changes are NOT tracked**. - Of course, because of this, we can use torch models in a normal situation without deep worries - Thus, with nn.Module or nn.Parameter, we can't get accurate gradients of $x$. Then, what can we use? **The answer is the functional F instead of nn.Module.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "            \n",
    "    def forward(self, x, w):\n",
    "        x = F.linear(x, w, bias=None)\n",
    "        return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.]])\n",
    "Y = torch.tensor([[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 th iteration]\n",
      "Grad: tensor([[-2.]])\n",
      "Weight: tensor([[1.2000]], grad_fn=<SubBackward0>)\n",
      "\n",
      "[ 1 th iteration]\n",
      "Grad: tensor([[-3.8400]])\n",
      "Weight: tensor([[1.4400]], grad_fn=<SubBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[1.]])\n",
    "w.requires_grad = True\n",
    "\n",
    "for j in range(2) :\n",
    "    print(\"[\",j, \"th iteration]\")\n",
    "    \n",
    "    pre = model(x, w)\n",
    "    cost = nn.L1Loss()(pre, Y)\n",
    "    \n",
    "    grad = torch.autograd.grad(cost, x, retain_graph=True)[0]\n",
    "    print(\"Grad:\", grad)\n",
    "    \n",
    "    grads = torch.autograd.grad(cost, w, \n",
    "                                retain_graph=True, create_graph=True)\n",
    "\n",
    "    w = w - 0.1*grads[i]\n",
    "    \n",
    "    print(\"Weight:\", w)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Right results for a meta learning framework!**. However, the problem is changing all the nn.Modules into functional is quite annoying. In this case, we can use the package [Higher](https://github.com/facebookresearch/higher). Thanks facebookresearch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(1, 1, bias=False)\n",
    "        self.fc.weight.data.uniform_(1,1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.]])\n",
    "Y = torch.tensor([[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 th iteration]\n",
      "Grad: tensor([[-2.]])\n",
      "Weight: tensor([[1.2000]], grad_fn=<AddBackward0>)\n",
      "\n",
      "[ 1 th iteration]\n",
      "Grad: tensor([[-3.8400]])\n",
      "Weight: tensor([[1.4400]], grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with higher.innerloop_ctx(model, optimizer) as (fmodel, diffopt):\n",
    "    for j in range(2):\n",
    "        print(\"[\",j, \"th iteration]\")\n",
    "        pre = fmodel(x)\n",
    "        cost = nn.L1Loss()(pre, Y)\n",
    "        \n",
    "        grad = torch.autograd.grad(cost, x, retain_graph=True)[0]\n",
    "        print(\"Grad:\", grad)\n",
    "        \n",
    "        diffopt.step(cost)\n",
    "        print(\"Weight:\", fmodel.fc.weight)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warning! the model is not changed.\n",
    "model.fc.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
